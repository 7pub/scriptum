## Immutability in Languages w/o Purely Functional Data Types

Immutability in a language without purely functional data types is a tough nut. It took me quite some time to explore the advantages and disadvantages and draw a few general conclusions. Here is my shot.

### Why mutability is harmful

Mutability is a side effect that introduces different kinds of potential errors to your code. The larger your codebase grows the harder it gets to keep them in check. As far as I know we can distinguish two distinct error classes:

* race conditions
* non-idempotence

The first point is simple. Race conditions exist in any language. In single threaded Javascript they can occur through the event loop, which allows asynchronous computations. I use a short example to clarify the second point:

```javascript
const arrCons_ = xs => x =>
  (xs.unshift(x), xs);

const empty = [];

const fold = f => acc => ([x, ...xs]) =>
  x === undefined
    ? acc
    : f(fold(f) (acc) (xs)) (x);
  
const xs = [1,2,3];

const main = fold(arrCons_) (empty);

main(xs);
main(xs); // [1,2,3,1,2,3]
```
[run code](https://repl.it/repls/LowQuixoticThing)

The `main` operation is not idempotend as expected and thus yields `[1,2,3,1,2,3]` instead of `[1,2,3]`.

### Why immutability is useful

Immutability does not magically make complexity disappear. First of all it represents another level of complexity all by itself. However, this kind of complexity can be mastered much better than subtle side effects, because it is based on principles more comprehensible for people. Hopefully I am able to convince you in the next sections that immutability is worth the hassle.

### Use data types according to their design

Javascript is based on native data types that are inherently mutable. You may treat them as immutable but it is at the expense of their efficiency:

```javascript
const take = n => ([x, ...xs]) =>
//                     ^^^^^
  n === 0
    ? []
    : [x].concat(take(n - 1) (xs));
//        ^^^^^^
    
const xs = Array(1e5).fill(1).map((x, i) => x + i);

take(200) (xs); // incredible inefficient
```
Both underlined lines of code are responsible for creating a brand new copy of the array for each iteration, while the previous one is just thrown away. This is ridiculously inefficient. Personally I think the rest syntax and `concat` are among the most harmful operations in Javascript, since they entice developers to misuse arrays.

It is crucial to understand that immutability in a multi-paradigm language is not a silver bullet. You must use data types in a way that they were designed to be used. Do not attampt to convert mutable data types into immutable ones.

### Keep mutations local

We can render the above example more efficient by relying on local mutability:

```javascript
const push = x => xs => (xs.push(x), xs);

const take = n => xs =>
  n < 0
    ? []
    : push(xs[n]) (take(n - 1) (xs));
    
const xs = Array(1e5).fill(1).map((x, i) => x + i);

take(200) (xs); // quite efficient
```
The mutation only takes place inside `take` and is not visible in the parent scope. Such mutations are managable, because race conditions and other potential errors are now delimited to the function scope.

### Copy as a last resort

Sometimes we cannot keep side effects from leaking into the parent scope. In these cases we have to copy the data structure once right before we start the computation. For the example from the beginning of this chapter copying just means to put the array literal in a function context, so that each invocation creates a new array instance:

```javascript
const arrCons_ = xs => x =>
  (xs.unshift(x), xs);

const arrEmpty = () => [];
//               ^^^^^^^^

const fold = f => acc => ([x, ...xs]) =>
  x === undefined
    ? acc()
//       ^^
    : f(fold(f) (acc) (xs)) (x);
  
const xs = [1,2,3];

const main = fold(arrCons_) (arrEmpty);

main(xs);
main(xs); // [1,2,3]
```
[run code](https://repl.it/repls/RaggedAccomplishedQueries)

### Reduce the need for immutability

A frequent scenario where immutability matters is gradual data acquisition. Imagine you successively query external data, process it and query further external data depending on the result of the previous processing step. If there are relations between both chunks of data you usually maintain them by arranging the data within a compound data type in a certain way. Since you do not have all data upfront, this data structure is gradually growing. On first sight there seem to be only two alternatives: Either mutate the data structure or copy it before each update.

However, a viable third alternative is to create a separate structure for each chunk of data and maintain relations between data chunks by sticking to a certain shape:

```javascript
const foo = {
  keyA: {
    keyB: [1, 2, 3],
    /*...*/
  },
  /*...*/
};

const bar = {
  keyA: {
    keyB: ["a", "b", "c"],
    /*...*/
  },
  /*...*/
};

const baz = {
  keyA: {
    keyB: [true, false, false],
    /*...*/
  },
  /*...*/
};
```
Given these three data structures it is quite easy to retrieve the relation between their data components: `[1, "a", true]`, `[2, "b", false]` and `[3, "c", false]`. While this approach works for settings with few updates it quickly gets convoluted as soon as the number of updates grow significantly.

### Immutable objects through functional lenses

Technically plain old Javascript objects are unordered maps based on hash tables. However, you should not use them as dictionaries but rather as plain records, which you can nest to form arbitrarily tree structures.

Nested objects have an interesting trait as soon as you access them with functional lenses to set, delete or update nodes. They act like persistent data structures. Hence there is no need for immutable objects. Lenses are the functional idiom of combined getters and setters. Since they are an advanced topic that depends on a couple of other functional techniques they will only be covered in a subsequent chapter of this course.

### Persistent data structures and structural sharing

Persistent data structures gain immutability by maintaining all previous states. When you change such a structure only the portion that is actually altered gets copied, whereas the rest is shared between instances. Since persistent data structures are organized as trees that means only the involved node including its nodes along the root pathe are copied. This technique is referred to as structural sharing and it enables efficient copying.

`Hamt` by the way, the persistent data structure used in this course is based on a hashed array mapped trie (HAMT). Besides `String`s it allows all sorts of types as keys, including `Object`s. It has a pretty straightforward API:

```javascript
Hamt(props)
// takes an object with arbitrary properties
// returns fresh empty Hamt with these extra properties

hamtGet(hamt, k)
// takes a hamt and a key of arbitrary type
// returns the corresponding value

hamtHas(hamt, k)
// takes a hamt and a key
// returns true if the key exists and false otherwise

hamtSet(hamt, props1, props2, k, v)
// takes a hamt, two objects, a key and a value
// returns a new hamt with the value set under the given key

hamtDel(hamt, props, k)
// takes a hamt, an object and a key
// returns a new hamt with the key removed

hamtUpd(hamt, props, k, f)
// takes a hamt, an object, a key and a pure function
// returns a new hamt with the updated value under the given key
```
The principle to modify such a persistent data structure is always the same: A function takes a `Hamt` instance and at least a key and returns a new `Hamt` with the modified key, in which all other keys are shared with previous instances.

#### An immutable array

We can gain a better feeling how to handle persistent data structures by having a glimpse at a specific implementation. Here is an excerpt from the `Iarray` API:

```javascript
const Iarray = Hamt(
  {length: 0, offset: 0});

const iarrCons = x => xs =>
  hamtSet(
    xs,
    {},
    {length: xs.length + 1, offset: xs.offset - 1},
    xs.offset - 1,
    x);

const iarrSnoc = x => xs =>
  hamtSet(
    xs,
    {},
    {length: xs.length + 1, offset: xs.offset},
    xs.length,
    x);

const iarrFold = f => acc => xs => {
  for (let i = 0; i < xs.length; i++)
    acc = f(acc) (hamtGet(xs, i + xs.offset), i);

  return acc;
};

const iarrFromArr = arrFold(
  acc => (x, i) =>
    hamtSet(acc, {}, {length: acc.length + 1, offset: 0}, i, x))
      (Iarray);

const iarrToArr = thunk_(() =>
  iarrFold(
    acc => (x, i) => arrSnoc(x) (acc))
      ([]));

const xs = iarrFromArr([1, 2, 3, 4, 5]);

iarrToArr(iarrCons(0) (xs)); // [0, 1, 2, 3, 4, 5]
iarrToArr(iarrSnoc(6) (xs)); // [1, 2, 3, 4, 5, 6]
```
[run code](https://repl.it/repls/MessyGlumHexadecimal)

`cons`/`snoc` are the immutable functional counterparts to `unshift`/`push` from the imperative world. It should be clearer now how the `props` argument of most `Hamt` functions is meant to be used. `hamtSet` expects even two of them, because setting actually consists of two operations, one to add a new key/value pair and another to modify an existing one.

You have probably already seen the biggest downside of `Hamt` and persistent data structures in Javascript in general: They are not native and therefore require a lot of transformations. `Hamt` is a tradeoff between performance gain through immutability on the one hand and performance loss through transformations on the other hand. As a rule of thumb we can state that the benefits outweigh their drawbacks when dealing with large amounts of data.

By the way, did you notice that I had to deal with the _non-idempotence_ error from the first example of this chapter? I wrapped `iarrToArr` in an implicit thunk to keep the operation idempotent. I had to take care of side effects and the errors they cause, because I treat arrays as a mutable data type. The crucial part is `arrSnoc`'s implementation `arrSnoc = x => xs => (xs.push(x), xs)`, which pushes elements onto the `Array`. See what happens if I [remove the implicit thunk](https://repl.it/repls/QuaintStableTests).

Now we are prepared to reimplement the `Array`-based and thus inefficient `take` function from the beginning of this chapter. We implement it with the very same algorithm but along with `Iarray` as its underlying data structure:

```javascript
const arrTake = n => ([x, ...xs]) =>
  n === 0
    ? []
    : [x].concat(arrTake(n - 1) (xs));

const iarrTake = n => xs => {
  const go = ([y, ys], m) =>
    m === 0
      ? Iarray
      : iarrCons(y) (go(iarrUncons(ys), m - 1));

  return go(iarrUncons(xs), n);
};

const xs = Array(1e5).fill(1).map((x, i) => x + i),
  ys = iarrFromArr(xs);

arrTake(200) (xs);
iarrTake(200) (ys); // way more efficient
```
[run code](https://repl.it/repls/ComplicatedInformalStatistics)

[&lt; prev chapter](https://github.com/kongware/scriptum/blob/master/ch-10.md) | [TOC](https://github.com/kongware/scriptum#functional-programming-course-toc)
