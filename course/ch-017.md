## Combining Effects with Actions using Monad

### The limitation of applicative

In the chapter about applicatives we have learned that the next applicative effect may depend on a previous one but we cannot determine the next effect depending on a previous dynamically evaluated value. It turns out that despite this limitation applicative computations are applicable for a surprising number of cases. They are not always enough though. We need a slightly less general, slightly more expressive alternative.

### Extending `Applicative` by `Monad`

Provided you have understood applicative functors it is only a small step to comprehend monads:

_Monads have essentially the purposes to overcome the limitation of applicatives._

This also means that you always should attempt to use applicatives first. Only when you cannot express something with this algebraic structure you should resort to the extra power of monads, because such power always entails its very own drawbacks. During this chapter we will come across some. There is also a common misunderstanding in connection with them:

_Monads are no means to handle I/O._

A purely functional language ships with a special type baked into its core to handle I/O and to connect the pure realm of the language with the real world during compilation. Monads as well as applicatives are merely a means to combine computations of this type. THis does not hold for Javascript, of course, because the language is untyped and we have to construct our own runtime.

#### From pure functions to actions

So far we lifted n-ary pure functions into the context of `n` applicative values. If we want to determine the next effect by the previous dynamic value, lifting a pure function is not enough. We need a way to intermingle the value and context level. In order to achieve this the function itself must return a value wrapped in the same effectful context as the corresponding monadic computation. Such a function is called an action. An action is not impure, but it may represent an effectful computation, provided it is used as part of an applicative or monadic computation:

```javascript
// Functor

const arrMap = f => xs =>
  xs.map((x, i) => f(x, i));

// Applicative

const arrAp = tf => xs =>
  arrFold(acc => f =>
    arrAppend(acc)
      (arrMap(x => f(x)) (xs)))
        ([])
          (tf);

// auxiliary functions

const arrFold = f => init => xs => {
  let acc = init;
  
  for (let i = 0; i < xs.length; i++)
    acc = f(acc) (xs[i], i);

  return acc;
};

const arrAppend = xs => ys =>
  (xs.push.apply(xs, ys), xs);

const comp = f => g => x => f(g(x));

// action

const foo = x => y =>
  y === null
    ? []
    : [x, y];

// MAIN

// result of the action interpreted as a value
arrAppend(
  foo(1) (2)) 
    (foo(3) (null)); // [1, 2, 3]

// result of the action interpreted as a computation
arrAp(
  arrMap(foo)
    ([1, 2]))
      ([3, null]); // [[1, 3], [], [2, 3], []]
```
[run code](https://repl.it/@scriptum/TubbyUnconsciousVolume)

When we use `foo` in a non-functorial context its result is treated like an ordinary value, which is a collection of values of natural numbers in the example above. However, when we use it as part of an applicative computation, the result is interpreted differently. It now represents a computation, namely a non-deterministic one. Since `foo` is an action that returns a value wrapped in an effectful context, the next effect can depend on the previous value. But now we are stuck with a nested context. The desired result is `[1, 3, 2, 3]`. Obviously actions are only half the solution.

#### The `join` operation

How about joining two effectful contexts, provided they are of the same type:

```javascript
// Monad

const arrJoin = xs =>
  arrFold(arrAppend) ([]) (xs);

// MAIN

arrJoin(
  arrMap(x =>
    arrJoin(
      arrMap(y =>
        y === null
          ? []
          : [x, y])
              ([3, null])))
                ([1, 2])); // [1, 3, 2, 3]
 ```
[run code](https://repl.it/@scriptum/FragrantSugaryObjectpool)

Have you seen how the computational structure has changed from a compositional to a nested form using closures? This is typical for a monadic computation. `join` is a sufficient operation and yields the expected result. Composing `map` with `join` is not particularly convenient though. Can we do better?

#### The `chain` operation

`chain` collapses the previous and the next context and thus combines `map` and `join` semantically:

```javascript
// Monad

const arrChain = mx => fm =>
  arrFold(acc => x =>
    arrAppend(acc) (fm(x))) ([]) (mx);

// MAIN

arrChain([1, 2]) (x =>
  arrChain([3, null]) (y =>
    y === null
      ? []
      : [x, y])); // [1, 3, 2, 3]
```
[run code](https://repl.it/@scriptum/HarmoniousAdorableScript)

In other functional languages this combinator is known as `flatmap`. You can transform `chain` to `join` and vice versa with the following laws:

```
= denotes equivalence
fm denotes an action
mx denotes a pure value x in an effectful context m
id denotes the identity function

join(mx) = chain(mx) (id)
chain(mx) (fm) = join(map(fm) (mx))
```
### Value/effect dependency

The characteristic property of monads is their ability to choose the next effect depending on a previous value. This feature presupposes that the next effect must depend on the previous one, because you cannot have one dependency without the other. Let us illustrate these dependencies in a more schematic manner. Given is an effectul computation `F<A>` where `A` is the result value of a previous effectful computation. It applies:

```
F   is an effectul context
A   is a pure value
~   denotes an x-may-depend-on-y relation
-   denotes an x-depends-on-y relation
</> denotes the direction of a dependency

Applicative constitutes:
F <~ F

Monad constitutes:
F <- F
A <~ F
```
Compared to applicatives this is a limitation of the monadic concept. We can demonstrate it using asynchronous computations that are run in parallel or in sequence:

```javascript
// record constructor

const record = (type, o) =>
  (o[type.name || type] = type.name || type, o);

// PARALLEL

const Parallel = par => record(
  Parallel,
  thisify(o => {
    o.par = (res, rej) =>
      par(x => {
        o.par = k => k(x);
        return res(x);
      }, rej);
    
    return o;
  }));

// Functor

const parMap = f => tx =>
  Parallel((res, rej) =>
    tx.par(x => res(f(x)), rej));

// Applicative

const parAp = tf => tx =>
  Parallel((res, rej) =>
    parAnd(tf) (tx)
      .par(([f, x]) =>
         res(f(x)), rej));

const parOf = x => Parallel((res, rej) => res(x));

// TASK

const Task = task => record(
  Task,
  thisify(o => {
    o.task = (res, rej) =>
      task(x => {
        o.task = k => k(x);
        return res(x);
      }, rej);
    
    return o;
  }));

// Functor

const taskMap = f => tx =>
  Task((res, rej) =>
    tx.task(x => res(f(x)), rej));

// Applicative

const taskAp = tf => tx =>
  Task((res, rej) =>
     tf.task(f =>
       tx.task(x =>
         res(f(x)), rej), rej));

const taskOf = x => Task((res, rej) => res(x));

// Monad

const taskChain = mx => fm =>
  Task((res, rej) =>
    mx.task(x =>
      fm(x).task(res, rej), rej));

// auxiliary functions

const id = x => x;
const thisify = f => f({});
const comp = f => g => x => f(g(x));
const add = x => y => x + y;

const delayParallel = f => ms => x =>
  Parallel((res, rej) => setTimeout(comp(res) (f), ms, x));

const delayTask = f => ms => x =>
  Task((res, rej) => setTimeout(comp(res) (f), ms, x));

// MAIN

const mainParallel = parAp(parMap(add)
  (delayParallel(id) (1000) (2)))
    (delayParallel(id) (1000) (3));

const mainTask = taskChain(
  delayTask(id) (1000) (2)) (x =>
    taskChain(delayTask(id) (1000) (3))
      (y => taskOf(x + y)));

// parallel addition: 2 * 1000ms = 1000ms
mainParallel.par(console.log); // logs 5 after 1000ms

// sequential addition: 2 * 1000ms = 2000ms
mainTask.task(console.log); // logs 5 after 2000ms
```
[run code](https://repl.it/@scriptum/LimeBraveFlatassembler)

`Task` performs its effect in sequence whereas `Parallel` runs it in parallel. `Task` implements `Monad` and thus also `Applicative`. `Parallel` on the other hand only implements `Applicative`, because a monad cannot perform its effects in parallel by design. You might wonder why I declared `Parallel` in the first place, instead of implementing `Task`'s `Applicative` instance with in parallel semantics. Having a monadic type whose `Applicative` behaves differently is considered bad practice. This is only a convention but a useful one that you should adhere to.

Although monadic effects are ususally performed in sequence, there are instances that exhibit a non-deterministic effect order. I still believe the idea of monads as effect sequencing operators is still helpful, even though It might be a simplification for some edge cases.

The attentive reader may have already noticed that a monad is not necessary for the above task, because the next effect does not depend on a previous value. Usually you should favor applicatives over monads for such computations, however, for the sake of comparability I used the monad machinery.

### Nested closures

The expressivness of monads arises from their special computational structure. They form nested closures so that an inner function has access to the free variables of outer ones:

```javascript
chain(mx) (x => chain(my) (y => chain(mz) (z => /* function body */))) // monadic function
           x =>            y =>            z => /* function body */    // curried function
```
A monadic computation basically shares the structure of a curried function, except that for each nesting there is an extra bit of computation. As opposed to applicative lifting a monadic computation does not lift anything but forms the action itself. Value and context layer are intertwined in order to allow the previous-value/next-effect dependency. There is no monadic chaining without nesting. You even cannot abstract from it, unless you use macros like Haskell's `do` notation or other pre-compilation steps.

### Essentially monadic
***
* array example
* function/reader

The special characteristic of monads is their ability to choose the next effect depending on a previous value. But what are the consequences of this dependency in practice? How does a result value of a monadic computation affect the next effect? What constitutes an essentially monadic computation? There is no general answer to this questions, because the essentially monadic aspect of an effectful context wildy varies across instances. We have already seen that the essentially monadic aspect of the asynchronous context encoded with the `Task` type is the inability to run in parallel. Let us examine other effectulf contexts and their associated types in this regard. 

#### Array instance

The essentially monadic aspect of non-deterministic arrays is their ability to form different shapes depending on dynamic values, that is, values which are only determined at runtime. Since an array is a linear data type different shapes mean different lengths:

```javascript
// action

const foo = x => y =>
  y === null
    ? []
    : [x, y];

// MAIN

// Apllicative

arrAp(
  arrMap(foo)
    ([1, 2]))
      ([3, 4]); // [[1, 3], [1, 4], [2, 3], [2, 4]]

arrAp(
  arrMap(foo)
    ([1, 2]))
      ([3, null]); // [[1, 3], [], [1, 4], []]

// Monadic

arrChain([1, 2]) (x =>
  arrChain([3, 4]) (y =>
    foo(x) (y))); // [1, 3, 1, 4, 2, 3, 2, 4]

arrChain([1, 2]) (x =>
  arrChain([3, null]) (y =>
    foo(x) (y))); // [1, 3, 1, 4]
```
[run code](https://repl.it/@scriptum/QueasyFrenchAnalysts)

With applicatives it does not matter what we do the length of the resulting arrays is determined at compile time and thus known upfront. In our example each applicative computation yields four-element arrays. The monadic computations on the other hand can yield arrays of different length depending on dynamic values which are only evaluated at runtime. In the above example I use merely static array literals for the sake of simplicity, but I could have used dynamic arrays and it would still work.

#### Function instance

The essentially monadic aspect of computations that share a common read-only environment is the ability to call a function none at all, once or several times:

```javascript
// FUNCTION

// Applicative

const funAp = tf => tg => x =>
  tf(x) (tg(x));

// Monad

const funChain = mg => fm => x =>
  fm(mg(x)) (x);

// auxiliary functions

const log = x => console.log(x);

const myDiv = env => x => y => {
  const r = x / y;
  if (env.debug) log(r);
  return r;
};

const mySqr = env => x => {
  const r = x * x;
  if (env.debug) log(r);
  return r;
};

// MAIN

const mainA = funAp(
  funAp(env =>
    env.y === 0
      ? _ => _ => _ => Infinity
//      ^^^^^^^^^ redundant function application
      : f => g => x => f(g(x)) (env.y))
//                     ^^^^^^ no explicit env passing
        (myDiv))
          (mySqr);

const mainM = funChain(myDiv) (f => env =>
  env.y === 0
    ? _ => Infinity
    : funChain(mySqr) (g => env =>
        x => f(g(x)) (env.y)) (env));
//           ^^^^^^ no explicit env passing

mainA({debug: true, y: 4}) (6); // logs 36, 9 and yields 9
mainA({debug: true, y: 0}) (6); // logs nothing and yields Infinity

mainM({debug: true, y: 4}) (6); // logs 36, 9 and yields 9
mainM({debug: true, y: 0}) (6); // logs nothing and yields Infinity
```
[run code](https://repl.it/@scriptum/UnpleasantViolentProlog)

TODO

### The two purposes of constructor invocations

There are two distinct purposes of constructor invocations in functional programming. An invocation like `[1, 2, 3]`, which happens to be in literal form in this example, can be used for the purpose of

* obtaining a collection of values of type natural number
* obtaining a non-deterministic computation that may yield none, one or many results

The first bullet point interprets `[1, 2, 3]` as an ordinary value and the second one as a computation with a specific computational effect, namely non-determinism in the given example. Depending on what purpose we pick we get completely different results:

***
```javascript
const arrAppend = xs => ys =>
  (xs.push.apply(xs, ys), xs);

const arrMap = f => xs =>
  xs.map((x, i) => f(x, i));

const arrAp = tf => xs =>
  arrReduce((acc, x) =>
    acc.concat(
      arrMap(x => f(x)) (xs)))
        ([])
          (tf);

// value notion
arrAppend([1, 2]) ([3, 4]); // [1, 2, 3, 4]

// computation notion
arrAp(
  arrMap(x => y => [x, y])) // A
    ([1, 2]) ([3, 4]); // [[1, 3], [1, 4], [2, 3], [2, 4]]
```
With the first application we just append two collections, which yields the union of both collections. The second application, however, yields all combinations of two non-deterministic computations. Please note that while the entire composition represents a non-deterministic computation, the array returned by the lambda in line `A` is again treated as a value. Mind-bending, right?

An applicative or the underlying functor respectively turns an ordinary value into a computation with a specific effect. You can think of applicative functors as semantics machines or little embedded effect-specific languages.

### Monads at the type level

* the reverse `chain` combinator is used for the sake of symetry
* `chain` is an application operator as well as `map`/`ap`

```
<   A, B>( f:   (x: A) => B ) => ( x:   A ) =>   B ; // function application
<F, A, B>( f:   (x: A) => B ) => (tx: F<A>) => F<B>; // functor lifting
<F, A, B>(tf: F<(x: A) => B>) => (tx: F<A>) => F<B>; // applicative lifting
<M, A, B>(ft: (x: A) => M<B>) => (tx: M<A>) => M<B>; // monadic binding
```
### Monadic laws

* left identity
* right identity
* associativity

### Action composition à la Kleisli

* show implementation of compk, reverse pipek (kleisli composition)

The reason the `chain` and not `kleisli` is part of the `Monad` API is that the both operations, which are associated to `Functor` and `Applicative`, are function application rather than function composition.

### Abstracting from nested application

### Recursion within a monad

* explain the difference between liftA2 and liftM2 (as opposed to Haskell)
* illustrate that liftM2 based on CPS is still more readable than deeply nested monad bindings
* show implementation of liftM2 combinator family
* show implementation of compk3 and pipek3 combinator families
* example with `Task` instance
* monad recursion
* monads allow also allow composition of actions
ma `bind` (f >=> g) = (ma `bind` f) `bind` g              -- bind = (>>=)
                    = (`bind` g) . (`bind` f) $ ma 
                    = join . fmap g . join . fmap f $ ma
with flipped >>=:
((g <=< f) =<<)  =  (g =<<) . (f =<<)  =  join . (g <$>) . join . (f <$>)
